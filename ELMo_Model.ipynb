{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\thrdl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#from baseline_models import get_words, get_nouns, get_noun_pairs, get_used_noun_frequency\n",
    "\n",
    "### old imports from base line models\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import random\n",
    "import re\n",
    "\n",
    "from collections import Counter, defaultdict \n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import inflect\n",
    "p = inflect.engine()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text(df):\n",
    "    texts = df.text.to_list() \n",
    "    \n",
    "    sentences = []\n",
    "    for sublist in texts:\n",
    "        sentence = []\n",
    "        for word in sublist:\n",
    "            word = word.lower()\n",
    "            if \"n't\" in word:\n",
    "                word = \"not\"\n",
    "            pattern = re.compile(r'[\\\"\\'`]')\n",
    "            if pattern.findall(word):\n",
    "                pass\n",
    "            else:\n",
    "                pattern = re.compile(r'[\\.\\?\\!;,]')\n",
    "                if pattern.findall(word):\n",
    "                    if len(sentence) > 1:\n",
    "                        sentences.append(\" \".join(sentence))\n",
    "                    sentence = []\n",
    "                else:\n",
    "                    flag = True\n",
    "                    for char in word:\n",
    "                        if ord(char) > 127:\n",
    "                            flag = False\n",
    "                            break\n",
    "                    if flag:\n",
    "                        sentence.append(word)\n",
    "        if len(sentence) > 0:\n",
    "            sentences.append(\" \".join(sentence))\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def tensor_label_array(embed,arrays_of_sentences_1, arrays_of_sentences_2):\n",
    "    tensor_label = []\n",
    "    for embedding, sentence in zip(embed, arrays_of_sentences_1):\n",
    "        if sentence in arrays_of_sentences_2:\n",
    "            tensor_label.append([embedding,1])\n",
    "        else:\n",
    "            tensor_label.append([embedding,0])\n",
    "    #print(tensor_label[:5])\n",
    "    return tensor_label\n",
    "\n",
    "def get_words(df):\n",
    "    sentences = df.text.to_list()    \n",
    "    flat_list = []\n",
    "    for sublist in sentences:\n",
    "        for item in sublist:\n",
    "            flat_list.append(item)           \n",
    "    return flat_list\n",
    "\n",
    "def get_noun_pairs(words):\n",
    "    nouns = []\n",
    "    noun_pairs = []\n",
    "    porter = PorterStemmer()\n",
    "    for i, (word, tag) in enumerate(words):\n",
    "        if \"n't\" in word:\n",
    "                word = \"not\"\n",
    "        \n",
    "        flag = True\n",
    "        for char in word:\n",
    "            if ord(char) > 127:\n",
    "                flag = False\n",
    "                break\n",
    "        if len(word) >= 3 and tag.startswith('NN') and flag:\n",
    "            if i > 0 and tag.startswith('NN') and (words[i-1][1].startswith('NN') or words[i-1][1].startswith('JJ') or words[i-1][1].startswith('PRP')):\n",
    "                w1 = word\n",
    "                w0 = \"\"\n",
    "                for char in words[i-1][0]:\n",
    "                    if ord(char) < 128:\n",
    "                        w0 += char\n",
    "                noun_pairs.append(w0+\" \"+w1)\n",
    "\n",
    "            else:\n",
    "                nouns.append(word)\n",
    "                #nouns.append(porter.stem(word)) #convert words to their basic form\n",
    "    return nouns, noun_pairs\n",
    "\n",
    "def evaluate(tensor_label, df, label):\n",
    "    accuracy = 0\n",
    "    \n",
    "    model = \"https://tfhub.dev/google/elmo/3\"\n",
    "    hub_layer = hub.KerasLayer(model, output_key=\"default\",\n",
    "                           trainable=False)\n",
    "    m = tf.keras.metrics.CosineSimilarity(axis=0)\n",
    "    \n",
    "    ctn = 0\n",
    "    for row in df.iterrows():\n",
    "        print(f\"word: {ctn}\")\n",
    "        ctn +=1\n",
    "        \n",
    "        words_1 = get_words(val_1)\n",
    "        tags_1 = nltk.pos_tag(words_1)\n",
    "        nouns_1, pairs_1 = get_noun_pairs(tags_1)\n",
    "        total_len = 0\n",
    "        \n",
    "        if len(pairs_1) > 0:\n",
    "            mixture = np.array(pairs_1, dtype=bytes)\n",
    "            total_len = len(pairs_1)\n",
    "        else:\n",
    "            mixture = np.array(nouns_1, dtype=bytes)\n",
    "            total_len = len(nouns_1)\n",
    "        \n",
    "    \n",
    "        embed_all = hub_layer(mixture)\n",
    "        embed_all = tf.slice(embed_all, [0, 0], [total_len, 300])\n",
    "        \n",
    "        vote = 0\n",
    "        for embed in embed_all:\n",
    "            cos_max = -1\n",
    "            label_vote = 0\n",
    "            for tensors in tensor_label:\n",
    "                m.update_state(embed, tensors[0])\n",
    "                new_cos = m.result().numpy()\n",
    "                if new_cos > cos_max:\n",
    "                    cos_max = new_cos\n",
    "                    label_vote = tensors[1]\n",
    "            if label_vote == 0:\n",
    "                vote -= 1\n",
    "            else:\n",
    "                vote += 1\n",
    "        if (label == 0 and vote < 0) or (label == 1 and vote >= 0):\n",
    "            accuracy += 1\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6986\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "600\n",
      "1100\n",
      "1600\n",
      "2100\n",
      "2600\n",
      "3100\n",
      "3600\n",
      "4100\n",
      "executed\n",
      "word: 0\n",
      "word: 1\n",
      "word: 2\n",
      "word: 3\n",
      "word: 4\n",
      "word: 5\n",
      "word: 6\n",
      "word: 7\n",
      "word: 8\n",
      "word: 9\n",
      "word: 10\n",
      "word: 11\n",
      "word: 12\n",
      "word: 13\n",
      "word: 14\n",
      "word: 15\n",
      "word: 16\n",
      "word: 17\n",
      "word: 18\n",
      "word: 19\n",
      "word: 20\n",
      "word: 21\n",
      "word: 22\n",
      "word: 23\n",
      "word: 24\n",
      "word: 25\n",
      "word: 26\n",
      "word: 27\n",
      "word: 28\n",
      "word: 29\n",
      "word: 30\n",
      "word: 31\n",
      "word: 32\n",
      "word: 33\n",
      "word: 34\n",
      "word: 35\n",
      "word: 36\n",
      "word: 37\n",
      "word: 38\n",
      "word: 39\n",
      "word: 40\n",
      "word: 41\n",
      "word: 42\n",
      "word: 43\n",
      "word: 44\n",
      "word: 45\n",
      "word: 46\n",
      "word: 47\n",
      "word: 48\n",
      "word: 49\n",
      "word: 50\n",
      "word: 51\n",
      "word: 52\n",
      "word: 53\n",
      "word: 54\n",
      "word: 55\n",
      "word: 56\n",
      "word: 57\n",
      "word: 58\n",
      "word: 59\n",
      "word: 60\n",
      "word: 61\n",
      "word: 62\n",
      "word: 63\n",
      "word: 64\n",
      "word: 65\n",
      "word: 66\n",
      "word: 67\n",
      "word: 68\n",
      "word: 69\n",
      "word: 70\n",
      "word: 71\n",
      "word: 72\n",
      "word: 73\n",
      "word: 74\n",
      "word: 75\n",
      "word: 76\n",
      "word: 77\n",
      "word: 78\n",
      "word: 79\n",
      "word: 80\n",
      "word: 81\n",
      "word: 82\n",
      "word: 83\n",
      "word: 84\n",
      "word: 85\n",
      "word: 86\n",
      "word: 87\n",
      "word: 88\n",
      "word: 89\n",
      "word: 90\n",
      "word: 91\n",
      "word: 92\n",
      "word: 93\n",
      "word: 94\n",
      "word: 95\n",
      "word: 96\n",
      "word: 97\n",
      "word: 98\n",
      "word: 99\n",
      "word: 100\n",
      "word: 101\n",
      "word: 102\n",
      "word: 103\n",
      "word: 104\n",
      "word: 105\n",
      "word: 106\n",
      "word: 107\n",
      "word: 108\n",
      "word: 109\n",
      "word: 110\n",
      "word: 111\n",
      "word: 112\n",
      "word: 113\n",
      "word: 114\n",
      "word: 115\n",
      "word: 116\n",
      "word: 0\n",
      "word: 1\n",
      "word: 2\n",
      "word: 3\n",
      "word: 4\n",
      "word: 5\n",
      "word: 6\n",
      "word: 7\n",
      "word: 8\n",
      "word: 9\n",
      "word: 10\n",
      "word: 11\n",
      "word: 12\n",
      "word: 13\n",
      "word: 14\n",
      "word: 15\n",
      "word: 16\n",
      "word: 17\n",
      "word: 18\n",
      "word: 19\n",
      "word: 20\n",
      "word: 21\n",
      "word: 22\n",
      "word: 23\n",
      "word: 24\n",
      "word: 25\n",
      "word: 26\n",
      "word: 27\n",
      "word: 28\n",
      "word: 29\n",
      "word: 30\n",
      "word: 31\n",
      "word: 32\n",
      "word: 33\n",
      "word: 34\n",
      "word: 35\n",
      "word: 36\n",
      "word: 37\n",
      "word: 38\n",
      "word: 39\n",
      "word: 40\n",
      "word: 41\n",
      "word: 42\n",
      "word: 43\n",
      "word: 44\n",
      "word: 45\n",
      "word: 46\n",
      "word: 47\n",
      "word: 48\n",
      "word: 49\n",
      "word: 50\n",
      "word: 51\n",
      "word: 52\n",
      "word: 53\n",
      "word: 54\n",
      "word: 55\n",
      "word: 56\n",
      "word: 57\n",
      "word: 58\n",
      "word: 59\n",
      "word: 60\n",
      "word: 61\n",
      "word: 62\n",
      "word: 63\n",
      "word: 64\n",
      "word: 65\n",
      "word: 66\n",
      "word: 67\n",
      "word: 68\n",
      "word: 69\n",
      "word: 70\n",
      "word: 71\n",
      "word: 72\n",
      "word: 73\n",
      "word: 74\n",
      "word: 75\n",
      "word: 76\n",
      "word: 77\n",
      "word: 78\n",
      "word: 79\n",
      "word: 80\n",
      "word: 81\n",
      "word: 82\n",
      "word: 83\n",
      "word: 84\n",
      "word: 85\n",
      "word: 86\n",
      "word: 87\n",
      "word: 88\n",
      "word: 89\n",
      "word: 90\n",
      "word: 91\n",
      "word: 92\n",
      "word: 93\n",
      "word: 94\n",
      "word: 95\n",
      "word: 96\n",
      "word: 97\n",
      "word: 98\n",
      "validation err: 0.5416666666666666\n"
     ]
    }
   ],
   "source": [
    "topics = { \"abortion\": [\"abortion_pro_choice.csv\", \"abortion_pro_life.csv\"], \n",
    "           #\"gay_marriage\":[\"gay_marriage_for.csv\", \"gay_marriage_against.csv\"],\n",
    "           #\"darwin_theory_of_evolution\" :[\"darwin_theory_of_evolution_for.csv\", \"darwin_theory_of_evolution_against.csv\"],\n",
    "          #\"marijuana_legalization\" :[\"marijuana_legalization_against.csv\", \"marijuana_legalization_for.csv\"],\n",
    "         }\n",
    "\n",
    "\n",
    "for key, pair in topics.items():\n",
    "\n",
    "    df_1 = pd.read_csv(f\"./dataset_processed/{pair[0]}\", converters={2:ast.literal_eval})\n",
    "    df_2 = pd.read_csv(f\"./dataset_processed/{pair[1]}\", converters={2:ast.literal_eval})\n",
    "    \n",
    "    tr_1, ts_1 = train_test_split(df_1, test_size=0.2, random_state=42)\n",
    "    tr_2, ts_2 = train_test_split(df_2, test_size=0.2, random_state=42)\n",
    "    \n",
    "    tr_1, val_1 = train_test_split(tr_1, test_size=0.25, random_state=25)\n",
    "    tr_2, val_2 = train_test_split(tr_2, test_size=0.25, random_state=25)\n",
    "    \n",
    "    arrays_of_sentences_1 = prepare_text(tr_1)\n",
    "    arrays_of_sentences_2 = prepare_text(tr_2)\n",
    "    \n",
    "    arrays_of_sentences_1.extend(arrays_of_sentences_2)\n",
    "    random.shuffle(arrays_of_sentences_1)\n",
    "    \n",
    "    #mixture = arrays_of_sentences_1\n",
    "    mixture = np.array(arrays_of_sentences_1, dtype=bytes)\n",
    "    \n",
    "    print(len(mixture))\n",
    "    \n",
    "    #url = \"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\"\n",
    "    #embed = hub.load(url)\n",
    "    \n",
    "    model = \"https://tfhub.dev/google/elmo/3\"\n",
    "    hub_layer = hub.KerasLayer(model, output_key=\"default\",\n",
    "                           trainable=False)\n",
    "    \n",
    "    embed_all = hub_layer(mixture[:100])\n",
    "    embed_all = tf.slice(embed_all, [0, 0], [100, 300]) \n",
    "    \n",
    "    for i in range(100, len(mixture)-2500, 500):\n",
    "        print(i)\n",
    "        embed = hub_layer(mixture[i:i+500])\n",
    "        embed = tf.slice(embed, [0, 0], [500, 300]) \n",
    "        embed_all = tf.concat([embed_all, embed], 0) \n",
    "    print(\"executed\")\n",
    "    tensor_label = tensor_label_array(embed,arrays_of_sentences_1, arrays_of_sentences_2)\n",
    "    \n",
    "    acc_1 = evaluate(tensor_label, val_1, 0)\n",
    "    acc_2 = evaluate(tensor_label, val_2, 1)\n",
    "    \n",
    "    print(f\"validation err: {(acc_1+acc_2)/(len(val_1.index)+len(val_2.index))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
